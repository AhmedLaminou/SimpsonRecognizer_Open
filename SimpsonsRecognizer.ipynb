{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":27569,"sourceType":"datasetVersion","datasetId":1408}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2 as cv\nimport gc\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keras/TensorFlow Imports\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom, RandomTranslation\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------------------------\n# 1. SETUP VARIABLES AND DATA PATHS\n# -------------------------------------------------------------------\nIMG_SIZE = (80, 80)\nchannels = 1  # 1 for grayscale, 3 for color (RGB)\nchar_path = r'../input/the-simpsons-characters-dataset/simpsons_dataset'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a character dictionary and sorting it\nchar_dict = {}\nfor char in os.listdir(char_path):\n    # Only process directories\n    if os.path.isdir(os.path.join(char_path, char)):\n        try:\n            char_dict[char] = len(os.listdir(os.path.join(char_path, char)))\n        except Exception:\n            # Handle permissions or other errors if necessary\n            pass\n\n# Sort in descending order (Standard Python Replacement for caer.sort_dict)\nsorted_char_dict_list = sorted(char_dict.items(), key=lambda item: item[1], reverse=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select the top 10 characters to use as classes (common for this dataset)\ncharacters = [item[0] for item in sorted_char_dict_list[:10]]\nnum_classes = len(characters)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of classes selected: {num_classes}\")\nprint(f\"Classes: {characters}\")\nprint(\"-\" * 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------------------------\n# 2. CUSTOM PREPROCESSING FUNCTION (Replacement for caer.preprocess_from_dir)\n# -------------------------------------------------------------------\n\ndef preprocess_images_custom(DIR, classes, IMG_SIZE, channels):\n    data = []\n    \n    # Create a mapping from class name to numerical index (0 to num_classes-1)\n    class_to_index = {class_name: i for i, class_name in enumerate(classes)}\n    \n    print(\"[INFO] Starting preprocessing...\")\n\n    for class_name in tqdm(classes, desc=\"Processing Classes\"):\n        class_path = os.path.join(DIR, class_name)\n        class_index = class_to_index[class_name]\n        \n        if not os.path.exists(class_path):\n            continue\n            \n        # Iterate over all files in the class directory\n        for img_name in os.listdir(class_path):\n            try:\n                img_path = os.path.join(class_path, img_name)\n                \n                # Load image using OpenCV\n                # Use cv.IMREAD_GRAYSCALE for channels=1, or cv.IMREAD_COLOR for channels=3\n                if channels == 1:\n                    img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n                else:\n                    # By default, load color and convert to RGB (OpenCV loads BGR)\n                    img = cv.imread(img_path)\n                    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n                \n                if img is None:\n                    continue\n                    \n                # Resize image\n                resized_img = cv.resize(img, IMG_SIZE)\n                \n                # Append the image (features) and the class index (label)\n                data.append([resized_img, class_index])\n                \n            except Exception as e:\n                # print(f\"Error loading {img_name}: {e}\")\n                pass\n    \n    print(\"[INFO] Preprocessing complete. Shuffling data...\")\n    # Shuffle the data\n    np.random.shuffle(data)\n    \n    # Separate features (X) and labels (y)\n    X = np.array([item[0] for item in data])\n    y = np.array([item[1] for item in data])\n    \n    # Reshape features to (N, H, W, C)\n    # The -1 means infer the number of samples (N)\n    X = X.reshape(-1, IMG_SIZE[0], IMG_SIZE[1], channels)\n    \n    # Normalize features\n    X = X.astype('float32') / 255.0\n    \n    # Convert labels to categorical (one-hot encoding)\n    y = to_categorical(y, num_classes=len(classes))\n    \n    print(f\"[INFO] Features shape: {X.shape}\")\n    print(f\"[INFO] Labels shape: {y.shape}\")\n    \n    return X, y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------------------------\n# 3. IMPROVED MODEL FUNCTION - REDUCED COMPLEXITY + DATA AUGMENTATION\n# -------------------------------------------------------------------\n\ndef create_improved_simpsons_classifier(IMG_SIZE, channels, num_classes):\n    \"\"\"\n    Improved model with:\n    - Data augmentation layers\n    - Reduced model complexity (fewer filters)\n    - Increased dropout rates\n    - Batch normalization for stability\n    \"\"\"\n    model = Sequential()\n    \n    # DATA AUGMENTATION LAYERS (applied during training only)\n    model.add(RandomFlip(\"horizontal\", input_shape=(IMG_SIZE[0], IMG_SIZE[1], channels)))\n    model.add(RandomRotation(0.15))  # ±15 degrees\n    model.add(RandomZoom(0.1))  # ±10% zoom\n    model.add(RandomTranslation(height_factor=0.1, width_factor=0.1))  # ±10% translation\n    \n    # CONVOLUTIONAL LAYERS - REDUCED COMPLEXITY\n    # Layer 1: 16 filters (reduced from 32)\n    model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.4))  # Increased from 0.25\n    \n    # Layer 2: 32 filters (reduced from 64)\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.4))  # Increased from 0.25\n\n    # Layer 3: 64 filters (reduced from 128)\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.4))  # Increased from 0.25\n    \n    # CLASSIFICATION HEAD - REDUCED COMPLEXITY\n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))  # Reduced from 512\n    model.add(BatchNormalization())\n    model.add(Dropout(0.6))  # Increased from 0.5\n    model.add(Dense(num_classes, activation='softmax'))  # Output layer\n    \n    # Compile the model\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------------------------\n# 4. EXECUTION\n# -------------------------------------------------------------------\n\n# 4.1 Create the training data\nX_train, y_train = preprocess_images_custom(char_path, characters, IMG_SIZE, channels)\n\n# Clear memory\ngc.collect()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.2 Create the improved model\nmodel = create_improved_simpsons_classifier(IMG_SIZE, channels, num_classes)\n\n# Display model architecture\nprint(\"=\"*50)\nprint(\"IMPROVED MODEL ARCHITECTURE\")\nprint(\"=\"*50)\nmodel.summary()\nprint(\"=\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.3 Setup training configuration with callbacks\nBATCH_SIZE = 32\nEPOCHS = 30  # Increased from 10, but early stopping will prevent overfitting\n\n# CALLBACKS FOR BETTER TRAINING\ncallbacks = [\n    # Early Stopping: Stop training when validation loss stops improving\n    EarlyStopping(\n        monitor='val_loss',\n        patience=5,  # Stop if no improvement for 5 epochs\n        restore_best_weights=True,\n        verbose=1\n    ),\n    \n    # Model Checkpoint: Save the best model based on validation accuracy\n    ModelCheckpoint(\n        'simpsons_classifier_best.h5',\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max',\n        verbose=1\n    ),\n    \n    # Reduce Learning Rate: Reduce LR when validation loss plateaus\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,  # Reduce LR by half\n        patience=3,  # Wait 3 epochs before reducing\n        min_lr=1e-7,\n        verbose=1\n    )\n]\n\nprint(\"=\"*50)\nprint(\"TRAINING CONFIGURATION\")\nprint(\"=\"*50)\nprint(f\"Batch Size: {BATCH_SIZE}\")\nprint(f\"Max Epochs: {EPOCHS}\")\nprint(f\"Validation Split: 20%\")\nprint(f\"Callbacks: Early Stopping, Model Checkpoint, ReduceLROnPlateau\")\nprint(\"=\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.4 Train the model\nprint(\"\\n\" + \"=\"*50)\nprint(\"STARTING TRAINING\")\nprint(\"=\"*50)\n\nhistory = model.fit(\n    X_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_split=0.2,\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.5 Plot training history\nimport matplotlib.pyplot as plt\n\n# Plot accuracy\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\n# Plot loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Print final metrics\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL TRAINING METRICS\")\nprint(\"=\"*50)\nprint(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f} ({history.history['accuracy'][-1]*100:.2f}%)\")\nprint(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f} ({history.history['val_accuracy'][-1]*100:.2f}%)\")\nprint(f\"Final Training Loss: {history.history['loss'][-1]:.4f}\")\nprint(f\"Final Validation Loss: {history.history['val_loss'][-1]:.4f}\")\nprint(f\"Accuracy Gap: {(history.history['accuracy'][-1] - history.history['val_accuracy'][-1])*100:.2f}%\")\nprint(\"=\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clear training data from memory\ndel X_train\ndel y_train\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.6 Prediction Utility (Replacement for 'prepare' function using caer)\ndef prepare_image(img_path, IMG_SIZE, channels):\n    try:\n        if channels == 1:\n            img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n        else:\n            img = cv.imread(img_path)\n            img = cv.cvtColor(img, cv.COLOR_BGR2RGB)  # Convert BGR to RGB\n            \n        if img is None:\n            raise FileNotFoundError(f\"Image not found at {img_path}\")\n            \n        resized_img = cv.resize(img, IMG_SIZE)\n        \n        # Reshape for Keras (1, H, W, C) and normalize\n        reshaped_img = resized_img.reshape(1, IMG_SIZE[0], IMG_SIZE[1], channels) / 255.0\n        \n        return reshaped_img\n    except Exception as e:\n        print(f\"Error in prepare_image: {e}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.7 Example Prediction and Saving\n# Example test image path - uses first image from first character class\ntest_img_path = os.path.join(char_path, characters[0], os.listdir(os.path.join(char_path, characters[0]))[0])\n\n# Prepare the image\nprepared_img = prepare_image(test_img_path, IMG_SIZE, channels)\n\nif prepared_img is not None:\n    # Make prediction\n    predictions = model.predict(prepared_img)\n\n    # Get class with the highest probability\n    predicted_index = np.argmax(predictions[0])\n    predicted_class = characters[predicted_index]\n    confidence = predictions[0][predicted_index]\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"SAMPLE PREDICTION\")\n    print(\"=\"*50)\n    print(f\"Test Image: {test_img_path}\")\n    print(f\"Predicted Character: {predicted_class}\")\n    print(f\"Confidence: {confidence*100:.2f}%\")\n    print(\"=\"*50)\n\n    # Save the final model\n    model.save('simpsons_classifier.h5')\n    print(\"\\n[INFO] Model saved to 'simpsons_classifier.h5'\")\n    print(\"[INFO] Best model saved to 'simpsons_classifier_best.h5' (via ModelCheckpoint)\")\nelse:\n    print(\"Skipping prediction and saving due to image loading error.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}