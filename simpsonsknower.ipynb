{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":27569,"sourceType":"datasetVersion","datasetId":1408}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2 as cv\nimport gc\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keras/TensorFlow Imports\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.layers import RandomFlip, RandomRotation\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------------------------\n# 1. SETUP VARIABLES AND DATA PATHS\n# -------------------------------------------------------------------\nIMG_SIZE = (80, 80)\nchannels = 1  # 1 for grayscale, 3 for color (RGB)\nchar_path = r'../input/the-simpsons-characters-dataset/simpsons_dataset'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating a character dictionary and sorting it\nchar_dict = {}\nfor char in os.listdir(char_path):\n    # Only process directories\n    if os.path.isdir(os.path.join(char_path, char)):\n        try:\n            char_dict[char] = len(os.listdir(os.path.join(char_path, char)))\n        except Exception:\n            # Handle permissions or other errors if necessary\n            pass\n\n# Sort in descending order (Standard Python Replacement for caer.sort_dict)\nsorted_char_dict_list = sorted(char_dict.items(), key=lambda item: item[1], reverse=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select the top 10 characters to use as classes (common for this dataset)\ncharacters = [item[0] for item in sorted_char_dict_list[:10]]\nnum_classes = len(characters)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of classes selected: {num_classes}\")\nprint(f\"Classes: {characters}\")\nprint(\"-\" * 30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------------------------\n# 2. CUSTOM PREPROCESSING FUNCTION (Replacement for caer.preprocess_from_dir)\n# -------------------------------------------------------------------\n\ndef preprocess_images_custom(DIR, classes, IMG_SIZE, channels):\n    data = []\n    \n    # Create a mapping from class name to numerical index (0 to num_classes-1)\n    class_to_index = {class_name: i for i, class_name in enumerate(classes)}\n    \n    print(\"[INFO] Starting preprocessing...\")\n\n    for class_name in tqdm(classes, desc=\"Processing Classes\"):\n        class_path = os.path.join(DIR, class_name)\n        class_index = class_to_index[class_name]\n        \n        if not os.path.exists(class_path):\n            continue\n            \n        # Iterate over all files in the class directory\n        for img_name in os.listdir(class_path):\n            try:\n                img_path = os.path.join(class_path, img_name)\n                \n                # Load image using OpenCV\n                # Use cv.IMREAD_GRAYSCALE for channels=1, or cv.IMREAD_COLOR for channels=3\n                if channels == 1:\n                    img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n                else:\n                    # By default, load color and convert to RGB (OpenCV loads BGR)\n                    img = cv.imread(img_path)\n                    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n                \n                if img is None:\n                    continue\n                    \n                # Resize image\n                resized_img = cv.resize(img, IMG_SIZE)\n                \n                # Append the image (features) and the class index (label)\n                data.append([resized_img, class_index])\n                \n            except Exception as e:\n                # print(f\"Error loading {img_name}: {e}\")\n                pass\n    \n    print(\"[INFO] Preprocessing complete. Shuffling data...\")\n    # Shuffle the data\n    np.random.shuffle(data)\n    \n    # Separate features (X) and labels (y)\n    X = np.array([item[0] for item in data])\n    y = np.array([item[1] for item in data])\n    \n    # Reshape features to (N, H, W, C)\n    # The -1 means infer the number of samples (N)\n    X = X.reshape(-1, IMG_SIZE[0], IMG_SIZE[1], channels)\n    \n    # Normalize features\n    X = X.astype('float32') / 255.0\n    \n    # Convert labels to categorical (one-hot encoding)\n    y = to_categorical(y, num_classes=len(classes))\n    \n    print(f\"[INFO] Features shape: {X.shape}\")\n    print(f\"[INFO] Labels shape: {y.shape}\")\n    \n    return X, y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------------------------\n# 3. BALANCED MODEL - OPTIMAL CAPACITY + MODERATE REGULARIZATION\n# -------------------------------------------------------------------\n\ndef create_balanced_simpsons_classifier(IMG_SIZE, channels, num_classes):\n    \"\"\"\n    Balanced model with:\n    - Light data augmentation (flip + gentle rotation only)\n    - Moderate model complexity (32 -> 64 -> 96 filters)\n    - Moderate dropout rates (0.3 conv, 0.5 dense)\n    - Batch normalization for stability\n    \"\"\"\n    model = Sequential()\n    \n    # LIGHT DATA AUGMENTATION (only gentle transformations)\n    model.add(RandomFlip(\"horizontal\", input_shape=(IMG_SIZE[0], IMG_SIZE[1], channels)))\n    model.add(RandomRotation(0.1))  # ¬±10 degrees (reduced from ¬±15)\n    \n    # CONVOLUTIONAL LAYERS - BALANCED COMPLEXITY\n    # Block 1: 32 filters (keep original capacity)\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))  # Moderate dropout\n    \n    # Block 2: 64 filters (keep original capacity)\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))  # Moderate dropout\n\n    # Block 3: 96 filters (slight reduction from 128)\n    model.add(Conv2D(96, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(96, (3, 3), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.3))  # Moderate dropout\n    \n    # CLASSIFICATION HEAD - BALANCED CAPACITY\n    model.add(Flatten())\n    model.add(Dense(384, activation='relu'))  # Moderate reduction from 512\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))  # Standard dropout for dense layer\n    model.add(Dense(num_classes, activation='softmax'))  # Output layer\n    \n    # Compile with Adam optimizer\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=Adam(learning_rate=0.001),\n        metrics=['accuracy']\n    )\n    \n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------------------------------------------\n# 4. EXECUTION\n# -------------------------------------------------------------------\n\n# 4.1 Create the training data\nX_train, y_train = preprocess_images_custom(char_path, characters, IMG_SIZE, channels)\n\n# Clear memory\ngc.collect()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.2 Create the balanced model\nmodel = create_balanced_simpsons_classifier(IMG_SIZE, channels, num_classes)\n\n# Display model architecture\nprint(\"=\"*60)\nprint(\"BALANCED MODEL ARCHITECTURE\")\nprint(\"=\"*60)\nmodel.summary()\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.3 Setup optimized training configuration\nBATCH_SIZE = 64  # Increased from 32 for more stable gradients\nEPOCHS = 30  # Max epochs with early stopping\n\n# OPTIMIZED CALLBACKS\ncallbacks = [\n    # Early Stopping: Monitor validation accuracy with more patience\n    EarlyStopping(\n        monitor='val_accuracy',  # Changed from val_loss\n        patience=7,  # Increased from 5 to give more time\n        restore_best_weights=True,\n        mode='max',\n        verbose=1\n    ),\n    \n    # Model Checkpoint: Save the best model\n    ModelCheckpoint(\n        'simpsons_classifier_best.h5',\n        monitor='val_accuracy',\n        save_best_only=True,\n        mode='max',\n        verbose=1\n    ),\n    \n    # Reduce Learning Rate: More conservative reduction\n    ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=4,  # Increased from 3\n        min_lr=1e-7,\n        verbose=1\n    )\n]\n\nprint(\"=\"*60)\nprint(\"OPTIMIZED TRAINING CONFIGURATION\")\nprint(\"=\"*60)\nprint(f\"Batch Size: {BATCH_SIZE} (increased for stability)\")\nprint(f\"Max Epochs: {EPOCHS}\")\nprint(f\"Validation Split: 20%\")\nprint(f\"Early Stopping: Monitor val_accuracy, patience=7\")\nprint(f\"Model Checkpoint: Save best val_accuracy\")\nprint(f\"Learning Rate: Start at 0.001, reduce on plateau\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.4 Train the balanced model\nprint(\"\\n\" + \"=\"*60)\nprint(\"STARTING TRAINING - BALANCED APPROACH\")\nprint(\"=\"*60)\nprint(\"Target: 90%+ validation accuracy with <5% train/val gap\")\nprint(\"=\"*60 + \"\\n\")\n\nhistory = model.fit(\n    X_train, y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_split=0.2,\n    callbacks=callbacks,\n    verbose=1\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.5 Plot training history\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 5))\n\n# Plot accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\nplt.axhline(y=0.90, color='g', linestyle='--', label='90% Target', alpha=0.7)\nplt.title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\n\n# Plot loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss', linewidth=2)\nplt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\nplt.title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate metrics\nfinal_train_acc = history.history['accuracy'][-1]\nfinal_val_acc = history.history['val_accuracy'][-1]\nfinal_train_loss = history.history['loss'][-1]\nfinal_val_loss = history.history['val_loss'][-1]\naccuracy_gap = (final_train_acc - final_val_acc) * 100\n\n# Find best validation accuracy\nbest_val_acc = max(history.history['val_accuracy'])\nbest_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL TRAINING METRICS\")\nprint(\"=\"*60)\nprint(f\"Final Training Accuracy:   {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\nprint(f\"Final Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\nprint(f\"Final Training Loss:       {final_train_loss:.4f}\")\nprint(f\"Final Validation Loss:     {final_val_loss:.4f}\")\nprint(f\"Accuracy Gap:              {accuracy_gap:.2f}%\")\nprint(\"-\" * 60)\nprint(f\"Best Validation Accuracy:  {best_val_acc:.4f} ({best_val_acc*100:.2f}%) at epoch {best_epoch}\")\nprint(\"=\"*60)\n\n# Success check\nif final_val_acc >= 0.90:\n    print(\"\\nüéâ SUCCESS! Validation accuracy >= 90%\")\nelif final_val_acc >= 0.85:\n    print(\"\\n‚úÖ GOOD! Validation accuracy >= 85%\")\nelif final_val_acc >= 0.80:\n    print(\"\\nüëç IMPROVED! Validation accuracy >= 80%\")\nelse:\n    print(\"\\n‚ö†Ô∏è  Still below 80% - may need further tuning\")\n\nif abs(accuracy_gap) < 5:\n    print(\"‚úÖ Excellent generalization (gap < 5%)\")\nelif abs(accuracy_gap) < 10:\n    print(\"üëç Good generalization (gap < 10%)\")\nelse:\n    print(\"‚ö†Ô∏è  Generalization gap still high\")\n\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clear training data from memory\ndel X_train\ndel y_train\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.6 Prediction Utility\ndef prepare_image(img_path, IMG_SIZE, channels):\n    try:\n        if channels == 1:\n            img = cv.imread(img_path, cv.IMREAD_GRAYSCALE)\n        else:\n            img = cv.imread(img_path)\n            img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n            \n        if img is None:\n            raise FileNotFoundError(f\"Image not found at {img_path}\")\n            \n        resized_img = cv.resize(img, IMG_SIZE)\n        reshaped_img = resized_img.reshape(1, IMG_SIZE[0], IMG_SIZE[1], channels) / 255.0\n        \n        return reshaped_img\n    except Exception as e:\n        print(f\"Error in prepare_image: {e}\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.7 Example Prediction and Model Saving\ntest_img_path = os.path.join(char_path, characters[0], os.listdir(os.path.join(char_path, characters[0]))[0])\n\nprepared_img = prepare_image(test_img_path, IMG_SIZE, channels)\n\nif prepared_img is not None:\n    predictions = model.predict(prepared_img, verbose=0)\n    predicted_index = np.argmax(predictions[0])\n    predicted_class = characters[predicted_index]\n    confidence = predictions[0][predicted_index]\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"SAMPLE PREDICTION TEST\")\n    print(\"=\"*60)\n    print(f\"Test Image: {os.path.basename(test_img_path)}\")\n    print(f\"Predicted Character: {predicted_class}\")\n    print(f\"Confidence: {confidence*100:.2f}%\")\n    print(\"=\"*60)\n\n    # Save the final model\n    model.save('simpsons_classifier.h5')\n    print(\"\\n[INFO] ‚úÖ Model saved to 'simpsons_classifier.h5'\")\n    print(\"[INFO] ‚úÖ Best model saved to 'simpsons_classifier_best.h5'\")\n    print(\"\\n\" + \"=\"*60)\nelse:\n    print(\"‚ö†Ô∏è  Skipping prediction due to image loading error.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
